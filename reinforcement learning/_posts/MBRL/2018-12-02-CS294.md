---
layout: post
title:  "RL"
categories: RL
tags: MBRL
comments: true
---

# [CS294 - 112 정리] Lecture2 - Supervised Learning and Imitation


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}



## Today’s Lecture

1. 순차적 의사결정 문제(sequential decision problem) 정의
2. Imitation learning: 의사결정을 위한 supervised learning.
    1. 직접 모방하는 작업?
    2. 더욱 잘, 자주 동작하는 방법?
3. (Deep) imitation learning에서 최근 연구의 case studies.
4. Imitation learning에서 빠진 부분? 
    * Reward 함수가 다음주부터 다뤄지는 이유

* 목표:
    * 정의 및 표기법 이해.
    * 기초적인 imitation learning 알고리즘 이해.
    * 강점 및 약점 이해.




## Terminology & notation

* 일반적인 __지도학습(supervised learning)__ 은 다음과 같다:
    * 입력: image
    * 출력: category
    * 중간: 학습하고자 하는 model, probability distribution(확률 분포)을 파라미터화하는 것 - conditional probability distribution(조건부확률분포).
        * 입력: image(observation)
        * 출력: categorical variable(label)
        * Theta: parameterized distribution의 파라미터


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/1.png" class="lead"   style="width:320px; height:=180px"/>
</figure>


__순차적 의사결정(Sequential decision making)문제__ 로 변경하기 위해 decision, sequential things를 추가

* Sequential 부분: t, time step
* decision 부분: a, 출력을 action으로 변경.


* $$\pi$$가 discrete할 수도 있고, continous할 수도 있음.
    * Discrete: real-value number
    * Continous: Gaussian distribution(mean, variance)
* Notation
    * observation: $$\sigma_t$$ 
    * action: $$a_t$$
    * policy: $$\pi_\theta (a_t \bar o_t)$$, conditional distribution
        * 일반적으로는 parially observed하기에 $$o_t$$로 주어진다
        * 만약 fully observed, $$o_t$$를 $$s_t$$로 대체.
    * state: $$s_t$$


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/3.png" class="lead"   style="width:320px; height:=240px"/>
</figure>


* State와 observation의 관계
    * _Observation_: state의 lossy consequence(손실된 결과)
    * _State_: 특성을 간단히 요약한 것이며 미래를 예측하는데 sufficient kind. 
  
> Fully obserbability: 컴퓨터는 입력을 pixel의 값으로만 구성, 물리적 특성을 가진 모멘텀 및 치타와 가젤의 위치를 알 수는 없다 - hidden


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/4.png" class="lead"   style="width:320px; height:=180px"/>
</figure>

> Partial obserbability : 치타가 차에 가려져 있다. state는 추론할 수 없더라도 위 상황과 동일(치타, 가젤의 물리적 특성은 존재하기 때문).

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/5.png" class="lead"   style="width:320px; height:=180px"/>
</figure>


* 이러한 관계를 graph(모델, dynamic Bayesian network)로 그리면 다음과 같다:
    * 각 시간에 따라 분리
        * policy($$\pi _ \theta$$) : o에서 a로 매핑, conditional probability distribution.
        * p($$s_{t+1} \bar s_t, a_t$$) : transition function, 현재의 state, action이 다음 state에 어떻게 영향을 끼치는 지 나타냄.
        * observation: state로부터 생성됨
    * State $$s_3$$는 $$s_1$$과는 독립적. 
        * 즉, __현재의 state만 있다면 추가적인(과거) 정보는 필요가 없다__.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/6.png" class="lead"   style="width:480px; height:=240px"/>
</figure>

-----------


## Imitation Learning


* __Behavior cloning__
    * 드라이버가 운전을 하며, 카메라 녹화 및 steering wheel을 encoding = data set.
    * 이를 이용하여 _supervised learning_ 을 통해 training(stochastic gradient descent 이용)하여 policy 도출.
  * 잘 동작? 일반적으로 No!!!!!!
      * 주어진 data set을 이용하여 진행하게 되면, 초반에는 잘 동작하지만 작은 차이가 발생해도 점점 차이가 커지며 발산하게 된다.
  

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/7.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/8.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


* 잘 동작하기 위해서 차량 앞에 추가적인 카메라를 설치하여 안정성 확보(수렴).
    * 각 카메라에서 결정되는 action을 서로 보상하여 steering - policy를 stabilize.
    * 노이즈를 추가(각 카메라)하고 이를 서로 보완하여 distribution 생성.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/9.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/10.png" class="lead"   style="width:480px; height:=240px"/>
</figure>


-   학습을 stabilizing controller(LQR)으로 모방 학습


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/11.png" class="lead"   style="width:480px; height:=240px"/>
</figure>


* 더욱 잘 동작하게 만드는 방법:
    * 문제점: 아래 사진과 같은 방법으로 모방 학습을 진행
        * $$p_{data}$$: 사람의 data set
        * $$p_{\pi_\theta}$$: 주어진 데이터와 차이가 발생.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/12.png" class="lead"   style="width:480px; height:=240px"/>
</figure>


* 해결책: $$p_{data} = p_{\pi_\theta}$$ 만들 수 있을까?
    * __DAgger: Dataset Aggregation__
        * 목표: $$p_{data}$$ 대신에 $$p_{\pi_\theta}$$에서 training data를 수집
        * 방법:  $$\pi_\theta$$실행, 그러나 label $$a_t$$가 필요.
            1. 사람의 데이터 $$D={o_1,a_1, …, o_N, a_N}$$을 모아서(사람이 직접 운전을 해서) $$\pi_\theta (a_t \bar o_t)$$를 학습.
            2. $$\pi_\theta(a_t \bar o_t)$$를 기반으로 실행하고, $$D_\pi = {o_1, …, o_M}$$ 을 얻는다.
            3. 사람이 직접 $$D_\pi$$에 대해 action label을 달아준다(외부개입).
            4. $$D \leftarrow D \cup D_\pi$$ , 원래의 데이터셋과 합쳐준다(aggregate).
            5. 1~4를 반복한다.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/13.png" class="lead"   style="width:480px; height:=360px"/>

</figure>

> 문제점 : 사람의 노동력(a lot of effort, right action)이 너무 많이 들어간다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC2/14.png" class="lead"   style="width:480px; height:=240px"/>
</figure>

---------








--------

# Reference
[CS294-112 Lecture1](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf)