<!DOCTYPE html>
    <html>
    <head>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
        <title>[CS294 - 112 정리] Lecture4 - Reinforcement Learning Introduction</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
        <link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        
        <script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
    </head>
    <body>
        <hr>
<h2 id="layout-posttitle-%22rl%22categories-rltags-mbrlcomments-true">layout: post<br>
title:  &quot;RL&quot;<br>
categories: RL<br>
tags: MBRL<br>
comments: true</h2>
<h1 id="cs294---112-%EC%A0%95%EB%A6%AC-lecture4---reinforcement-learning-introduction">[CS294 - 112 정리] Lecture4 - Reinforcement Learning Introduction</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<p>{:.no_toc}</p>
<ol>
<li>this unordered seed list will be replaced by toc as unordered list<br>
{:toc}</li>
</ol>
<h2 id="todays-lecture">Today’s Lecture</h2>
<ol>
<li>Definition of a Markov decision process</li>
<li>Definition of reinforcement learning</li>
<li>Anatomy of a RL algorithm</li>
<li>Brief overview of RL algorithm types</li>
</ol>
<ul>
<li>Goals:
<ul>
<li>definitions &amp; notation 이해.</li>
<li>중요한 RL 목적 이해.</li>
<li>이용 가능한 알고리즘의 요약.</li>
</ul>
</li>
</ul>
<h2 id="review">Review</h2>
<ul>
<li>Terminology &amp; notation
<ul>
<li>Sequential decision making 문제는 observation. action. policy를 포함.
<ul>
<li><strong>Policy</strong> 는 조건부 확률 분포이며, 일반적으로 observation이 주어지고 action이 도출된다. 파라미터화되어 있으며, 이 파라미터를 학습시키는 것이 RL의 목적이다. 바로 파라미터를 학습시키는 PG가 있다.</li>
<li>아래의 그래프에서 <strong>state</strong> 는 Markov property 의미를 만족, <strong>observation</strong> 은 Markov property를 만족하지 못함.</li>
<li>POMDPs과 Fully MDPs가 있는데, 일반적으로 fully MDPs로 가정한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li>
<p>Imitation Learning</p>
<ul>
<li>전문가의 데이터(observation, action)를 수집하고, supervised learning 알고리즘을 사용한다.
<ul>
<li>위와 같은 behavior cloning접근은 일반적으로 학습이 잘 되지않지만, Dagger를 사용하여 성능을 향상시켰다.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Reward functions</p>
<ul>
<li>전문가의 data가 없는 경우에, 어떻게 하면 효율적으로 policy를 학습할 수 있을까?</li>
<li>Reward를 이용하여 action이 좋은지 나쁜지를 찾을 수 있다.
<ul>
<li>Reward는 s, a로 구성, r(s, a)</li>
<li>Reward를 순간이 아닌 전체에 걸쳐 최대화 한다.</li>
<li>State, action, reward(r(s, a)), transition probability(p(s’$$\bar$$s,a))은 Markov decision process이라 한다.
<ul>
<li>MDP는 기본적으로 RL의 world의 정의.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="definitions">Definitions</h2>
<ul>
<li><strong>Markov chain</strong>
<ul>
<li>
T$$  - trainsition operator라고 한다. 현재 state에서 특정한 state로 가는 확률(조건부 확률분포).
  * 왜 operator라고 불리나?
      * Discrete space에서 $$T$$ 는 행렬이고, 확률의 vector($$\mu$$)에 대해 operator처럼 행동.
      * p는 state distribution이며, 0보다 큰 값을 가진다.
  * $$T _{i,j}$$ matrix는 $$p(s_{t+1} \bar s_t)$$, $$s_t$$에서 $$s_{t+1}$$로 전이하며, 간단한 linear equation으로 Markov chain forward $$(\overrightarrow{\mu_{t+1}} = T  \overrightarrow{\mu_t$$} ) 표현.

</li>
<li>Markov property로 인해 $$s_3$$는 $$s_1$$와 독립적.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/2.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li><strong>Markov decision process(MDP)</strong>
<ul>
<li>Reward, action 개념 도입.</li>
<li>Action, state가 다음 state를 결정.</li>
<li>Transition Operator는 tensor(3D) 형태를 가짐.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/3.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/4.png" class="lead"   style="width:480px; height:=120px"/>
</figure>
<ul>
<li><strong>Partially observed Markov decision process(POMDP)</strong>
<ul>
<li>MDP의 generalization model.</li>
<li>Observation, emission probability 도입</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/5.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<hr>
<h2 id="the-goal-of-reinforcement-learning">The goal of reinforcement learning</h2>
<ul>
<li>목적:
<ul>
<li>보상을 최대로하는 policy(π)의 weight(θ)찾기.</li>
<li>파라미터 vector θ* 찾기, trajectory에 걸쳐 보상의 총합의 theta로 유도된 trajectory distribution하에서 expectation의 argmax/</li>
</ul>
</li>
<li>p(s’ |s, a)는 일반적으로 모르는 상태이며, 확률이며 다음 state을 지정.</li>
<li>trajectory(sequence, τ)는 state, action으로 구성, chain rule을 이용하여 policy와 transition operator를 product로 표현.
<ul>
<li>Expectation을 사용하는 이유는 parameter vector인 state, action가 random variable이기 때문이고, 여기서의 분포를 사용.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/6.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<hr>
<h1 id="reference">Reference</h1>
<p><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf">CS294-112 Lecture1</a></p>

    </body>
    </html>