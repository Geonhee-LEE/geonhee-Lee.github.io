---
layout: post
title:  "RL"
categories: RL
tags: MBRL
comments: true
---

# [CS294 - 112 정리] Lecture4 - Reinforcement Learning Introduction


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}



## Today’s Lecture

1. Definition of a Markov decision process
2. Definition of reinforcement learning
3. Anatomy of a RL algorithm
4. Brief overview of RL algorithm types

* Goals:
    * definitions & notation 이해.
    * 중요한 RL 목적 이해.
    * 이용 가능한 알고리즘의 요약.



## Review

* Terminology & notation
  * Sequential decision making 문제는 observation. action. policy를 포함.
      * __Policy__ 는 조건부 확률 분포이며, 일반적으로 observation이 주어지고 action이 도출된다. 파라미터화되어 있으며, 이 파라미터를 학습시키는 것이 RL의 목적이다. 바로 파라미터를 학습시키는 PG가 있다.
      * 아래의 그래프에서 __state__ 는 Markov property 의미를 만족, __observation__ 은 Markov property를 만족하지 못함.
      * POMDPs과 Fully MDPs가 있는데, 일반적으로 fully MDPs로 가정한다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>



* Imitation Learning
  * 전문가의 데이터(observation, action)를 수집하고, supervised learning 알고리즘을 사용한다.
      * 위와 같은 behavior cloning접근은 일반적으로 학습이 잘 되지않지만, Dagger를 사용하여 성능을 향상시켰다.


* Reward functions
  * 전문가의 data가 없는 경우에, 어떻게 하면 효율적으로 policy를 학습할 수 있을까?
  * Reward를 이용하여 action이 좋은지 나쁜지를 찾을 수 있다.
    * Reward는 s, a로 구성, r(s, a)
    * Reward를 순간이 아닌 전체에 걸쳐 최대화 한다.
    * State, action, reward(r(s, a)), transition probability(p(s’$$\bar$$s,a))은 Markov decision process이라 한다.
        * MDP는 기본적으로 RL의 world의 정의.


--------

## Definitions

* __Markov chain__
  * $$T$$  - trainsition operator라고 한다. 현재 state에서 특정한 state로 가는 확률(조건부 확률분포).
      * 왜 operator라고 불리나?
          * Discrete space에서 $$T$$ 는 행렬이고, 확률의 vector($$\mu$$)에 대해 operator처럼 행동.
          * p는 state distribution이며, 0보다 큰 값을 가진다.
      * $$T _{i,j}$$ matrix는 $$p(s_{t+1} \bar s_t)$$, $$s_t$$에서 $$s_{t+1}$$로 전이하며, 간단한 linear equation으로 Markov chain forward $$(\overrightarrow{\mu_{t+1}} = T  \overrightarrow{\mu_t$$} ) 표현.
  * Continous space에서는 위와 같이 표현하기 어렵지만, computer에서는 기본적인 이론이 위와 같이 정확하게 동일하게 유지된다.
  * Markov property로 인해 $$s_3$$는 $$s_1$$와 독립적.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/2.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


* __Markov decision process(MDP)__
  * Reward, action 개념 도입.
  * Action, state가 다음 state를 결정.
  * Transition Operator는 tensor(3D) 형태를 가짐.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/3.png" class="lead"   style="width:560px; height:=360px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/4.png" class="lead"   style="width:560px; height:=120px"/>
</figure>


* __Partially observed Markov decision process(POMDP)__
  * MDP의 generalization model.
  * Observation, emission probability 도입

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/5.png" class="lead"   style="width:560px; height:=560px"/>
</figure>


--------


## The goal of reinforcement learning

* 목적:
    * 보상을 최대로하는 policy(π)의 weight(θ)찾기.
    * 파라미터 vector θ* 찾기, trajectory에 걸쳐 보상의 총합의 theta로 유도된 trajectory distribution하에서 expectation의 argmax/
* $$p(s’ \bar s, a)$$ 는 일반적으로 모르는 상태이며, 확률이며 다음 state을 지정.
* Trajectory(sequence, τ)는 state, action으로 구성, chain rule을 이용하여 policy와 transition operator를 product로 표현.
    * Expectation을 사용하는 이유는 parameter vector인 state, action가 random variable이기 때문이고, 여기서의 분포를 사용.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/6.png" class="lead"   style="width:480px; height:=360px"/>
</figure>







--------

# Reference
[CS294-112 Lecture1](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)