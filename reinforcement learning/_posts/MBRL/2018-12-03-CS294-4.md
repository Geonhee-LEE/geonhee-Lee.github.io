---
layout: post
title:  "RL"
categories: RL
tags: MBRL
comments: true
---

# [CS294 - 112 정리] Lecture4 - Supervised Learning and Imitation


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}



## Today’s Lecture

1. Definition of a Markov decision process
2. Definition of reinforcement learning
3. Anatomy of a RL algorithm
4. Brief overview of RL algorithm types

* Goals:
    * definitions & notation 이해.
    * 중요한 RL 목적 이해.
    * 이용 가능한 알고리즘의 요약.



## Review

* Terminology & notation
  * Sequential decision making 문제는 observation. action. policy를 포함.
      * __Policy__ 는 조건부 확률 분포이며, 일반적으로 observation이 주어지고 action이 도출된다. 파라미터화되어 있으며, 이 파라미터를 학습시키는 것이 RL의 목적이다. 바로 파라미터를 학습시키는 PG가 있다.
      * 아래의 그래프에서 __state__ 는 Markov property 의미를 만족, __observation__ 은 Markov property를 만족하지 못함.
      * POMDPs과 Fully MDPs가 있는데, 일반적으로 fully MDPs로 가정한다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>



* Imitation Learning
  * 전문가의 데이터(observation, action)를 수집하고, supervised learning 알고리즘을 사용한다.
      * 위와 같은 behavior cloning접근은 일반적으로 학습이 잘 되지않지만, Dagger를 사용하여 성능을 향상시켰다.


* Reward functions
  * 전문가의 data가 없는 경우에, 어떻게 하면 효율적으로 policy를 학습할 수 있을까?
  * Reward를 이용하여 action이 좋은지 나쁜지를 찾을 수 있다.
    * Reward는 s, a로 구성, r(s, a)
    * Reward를 순간이 아닌 전체에 걸쳐 최대화 한다.
    * State, action, reward(r(s, a)), transition probability(p(s’|s,a))은 Markov decision process이라 한다.
        * MDP는 기본적으로 RL의 world의 정의.


--------





--------

# Reference
[CS294-112 Lecture1](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)