---
layout: post
title:  "RL"
categories: RL
tags: MBRL
comments: true
---

# [CS294 - 112 정리] Lecture1 - Introduction and Course Overview


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}



### RL은 무엇이고 왜 다뤄야 하는가?

- 어떻게 지능 기계를 만들 수 있는가?
  - 실제 세상은 복잡하고 다양하며, 빠르게 반응을 해야한다.
  - 딥러닝은 구조화되지 않은 환경(=unstructured environment, 미리 정확하게 예상하지 못하는 환경)을 다룰 수 있는 도구.
    - 크고 강력한 모델을 설계하여 structured env를 다룰 수 있다.
    - Raw 센서 관측을 다룰 수 있다(수백만 파라미터를 가진 deep neural network 모델을 설계)

* __RL은 behavior에 대한 formalism을 제공__:
  * 이전에서 만들지 못했던 유연한 decision을 생성(decision making의 수학적 형태)
  * 원리는 간단: agent와 environment간의 action(decision), consequences(reward, observation)의 loop로 구성.

* __Deep RL__ 은 무엇인가? 무엇을 다뤄야하는가?
  * 일반적인 deep learning
    * 이전 computer vision 기술: pixel을 이용하여 분류, 특징점 추출하여 분류하는 방법 사용
    * Deep learning: multiple layer로 구성되어 있음(end-to-end 방식)
  * Deep learning과 RL의 조합 = Deep RL




### Sequential decision making에 대해 end-to-end 학습의 의미

* 현실적인 예제: robotics
    * 구동하기 위해서 필요한 pipeline이 상당히 많으며, 하나 하나가 연구 주제.
    * 복잡한 과정에서는 각각의 다양한 제약이 존재, Deep RL은 최종 결과(outcome)에 대해 최적화함으로써 극복.

* RL 문제들은 사실 머신 러닝 문제를 generalization 한 것.
    * Deep model은 복잡한 문제를 푸는 것.
    * RL은 이러한 문제를 형식화(공식화) 한 것.

### 어디서 reward가 오는가?

* Reward는 RL의 아주 기본적인 부분
    * 게임에서는 표시되는 score가 reward(단순)
    * 그러나 실제환경에서는 간단하지가 않고 score라는 것이 없음.
        * 논문 인용구: “인간 agent로서, 우리는 평생에 한번 혹은 두번만 경험하는, 희소한 보상을 가지고 수행하는 것에 익숙하다”


### 지도(supervision)의 다른 형태가 있는가?

* 시연(demonstration)에서의 학습
    * 관측된 행동을 직접 복사(behavior cloning)
    * 관측된 행동에서 reward 추론(inverse reinforcement learning)
* World를 관측하여 학습
    * Learning to predict(예측하기 위한 학습)
    * Unsupervised learning(비지도 학습)
* 다른 task로부터 학습
    * Transfer learning
    * Meta-learning: learning to learn

--------

# Reference
[CS294-112 Lecture1](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf)