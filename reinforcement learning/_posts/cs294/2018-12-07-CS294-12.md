---
layout: post
title:  "RL"
categories: RL
tags: CS294
comments: true
---

# [CS294 - 112 정리] Lecture12 - Advanced Model Learning and Images


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}

## Overview

1. Managing __overfitting in model-based RL__
    1. What’s the problem? 
    2. How do we represent uncertainty?
2. Model-based RL with __images__
    1. The POMDP model for model-based RL 
    2. Learning encodings 
    3. Learning dynamics-aware encoding
3. Goals: 
    1. Understand the issue with __overfitting and uncertainty__ in model-based RL 
    2. Understand how the POMDP model fits with model-based RL 
    3. Understand recent research on model-based RL with complex observations

------------------

## Problem: Overfitting

__A performance gap in model-based RL__
* Pure model-based는 작은 step(10분)에서도 학습이 진행되지만, _overfitting_ 으로 충분히 학습할수 있는 기회를 놓치게 된다(DNN은 big capacity를 가짐). 
    * Overfitting을 줄이기 위해 smaller model(=dynamics), regularization을 이용하기도 한다.
    * 그러나, 작은 모델을 선택하면 underfit 문제발생. 


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC12/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>

* Overfitting은 아래의 그림과 같고, model-based RL 1.5 버전에서 3번째 경우 action을 선택함(_adversary exploit, model이 optimistic하여 mistake)에 있어 __overfitting으로 인한 exploitation으로 좋은 성능을 얻지 못하게 되어 적당한 exploration을 필요__ 로 한다.
    * 아래 사진에서 보게되면 왼쪽 실제 reward는 작지만 오버피팅되어 reward이 큰 줄 알고 그쪽으로 갈려고 하게 됨.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC12/2.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


__Solution__

- Idea(Remember from last time…)
  * __Gaussian Process(GP, Bayesian model)__ 를 이용하여 dynamics를 구성함으로써 __uncertainty__(Gaussian distribution에서 next state에 포함되어짐)를 다룰수 있게 되었다.
    * Version 2.0 RL 알고리즘


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC12/3.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


* 이전에는 action을 선택하기 위해서는 dynamics를 이용하여 planning.
    * GP 모델을 사용하여 진행, __policy를 최적화(action 선택)하기 위해 dynamics를 backpropagation__.
    * 방법은 _moment matching_ 을 이용하여 non-Gaussian을 Gaussian으로 projection 시킨다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC12/4.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


__Why are GPs so popular for model-based RL?__
* GPs는 __uncertainty 추정을 잘한다__
    * 데이터가 주어진 곳(빨간점)이 아닌 지점은 불확실하다는 것을 보임
* 목표: 멋진 바다 절경을 보는 것(절벽에 가까워짐), 떨어지면 안됨
    * 작은 빨간 타원: 불확실성 고려 X
        * 가까이 접근
    * 큰 빨간 타원: 불확실성 고려
        * 절벽으로 접근하지 않음
    * 비록 평균이 같더라도 high-variance 하에서는 expected reward는 매우 작다.
        * 예를 들어, 절벽위에서 풍경을 보는 것이 가장 멋지지만 위험성이 크다.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC12/5.png" class="lead"   style="width:480px; height:=360px"/>
</figure>

------------------

# Reference
[CS294-112 Lecture12](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-12.pdf)