---
layout: post
title:  "RL"
categories: RL
tags: CS294
comments: true
---

# [CS294 - 112 정리] Lecture11 - Model-Based Reinforcement Learning


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}

## Overview

1. 지난 강의: 
    * 알고있는 system dynamics(e.g., physics)를 통해 backpropagating(or planning)하여 자체적으로 좋은 action을 선택.
2. 이번 강의: __dynamics__ 를 모른다면 어떻게 해야할까?
    1. Fitting global dynamics models ("model-based RL").
    2. Fitting local dynamics models.
3. 다음 강의:
    * Image들과 같이 높은 차원 observation에 대해 dynamics를 학습.
4. 다다음 강의:
    * Optimal control의 목표를 가지고 NN policies를 학습하기 위해 optimal control과 policy search 결합.


## Today’s Lecture

1. Overview of model-based RL
    1. Model만 학습
    2. Model & policy 학습
2. 어떤 모델을 사용할 수 있을까?
3. Global model & local model
4. Local model 및 trust region 학습

* Goals:
    * Model-based RL의 용어 및 구조 이해
    * Model-based RL에서 사용할 수 있는 모델 종류 이해
    * Model learning의 실제 고려사항 이해

* But, Deep RL은 이후에 다룰 예정.


-------


## Model-based reinforcement learning

Why learn the model?
- 아래의 빨간원으로 표현되는 미분 값을 알면 model을 이용할 수 있다.
- 즉, Deterministic case에서 $$f(s_t, a_t) = s_{t+1}$$을 안다면, 이전에 배웠던 __optimal control와 같은 방법(planning)__ 들을 사용할 수 있다
  - 다른 경우: stochastic case, $$p(s_{t+1} \mid s_t, a_t)$$.
- __Data로부터 $$f(s_t,a_t)$$을 학습하고, 이를 통해 planning__

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


* __Model-based reinforcement learning version 0.5:__
  1. Base policy $$π_0 (s_t, a_t)$$ (e.g., random policy)을 구동하여 D = {(s, a, s’)i} 수집.
  2. $$\sum_i \left \| f(s_i, a_i) - s' _i \right \| $$를 최소화하여 dynamics model f(s, a) 학습.
       1. Deterministic case: 위와 같이 regression loss.
       2. Stochstic인 case: maximizing the likelihood(log $$p(s' \bar a,s)$$).
       3. Gaussian distribution인 경우에는 동일함.
  3. Action을 선택하기 위해 _f(s, a)_ 를 통해 planning.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/2.png" class="lead"   style="width:480px; height:=360px"/>
</figure>

* 1, 2 과정은 supervised learning과 유사.
* 3 과정은 지난번에 했던 내용.







---------


# Reference
[CS294-112 Lecture11](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-11.pdf)