---
layout: post
title:  "RL"
categories: RL
tags: CS294
comments: true
---

# [CS294 - 112 정리] Lecture11 - Model-Based Reinforcement Learning


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}

## Overview

1. 지난 강의: 
    * 알고있는 system dynamics(e.g., physics)를 통해 backpropagating(or planning)하여 자체적으로 좋은 action을 선택.
2. 이번 강의: __dynamics__ 를 모른다면 어떻게 해야할까?
    1. Fitting global dynamics models ("model-based RL").
    2. Fitting local dynamics models.
3. 다음 강의:
    * Image들과 같이 높은 차원 observation에 대해 dynamics를 학습.
4. 다다음 강의:
    * Optimal control의 목표를 가지고 NN policies를 학습하기 위해 optimal control과 policy search 결합.


## Today’s Lecture

1. Overview of model-based RL
    1. Model만 학습
    2. Model & policy 학습
2. 어떤 모델을 사용할 수 있을까?
3. Global model & local model
4. Local model 및 trust region 학습

* Goals:
    * Model-based RL의 용어 및 구조 이해
    * Model-based RL에서 사용할 수 있는 모델 종류 이해
    * Model learning의 실제 고려사항 이해

* But, Deep RL은 이후에 다룰 예정.


-------


## Model-based reinforcement learning

Why learn the model?
- 아래의 빨간원으로 표현되는 미분 값을 알면 model을 이용할 수 있다.
- 즉, Deterministic case에서 $$f(s_t, a_t) = s_{t+1}$$을 안다면, 이전에 배웠던 __optimal control와 같은 방법(planning)__ 들을 사용할 수 있다
  - 다른 경우: stochastic case, $$p(s_{t+1} \mid s_t, a_t)$$.
- __Data로부터 $$f(s_t,a_t)$$을 학습하고, 이를 통해 planning__

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


* __Model-based reinforcement learning version 0.5:__
  1. Base policy $$π_0 (s_t, a_t)$$ (e.g., random policy)을 구동하여 D = {(s, a, s’)i} 수집.
  2. $$\sum_i \left \| f(s_i, a_i) - s' _i \right \| $$를 최소화하여 dynamics model f(s, a) 학습.
       1. Deterministic case: 위와 같이 regression loss(MSE).
       2. Stochstic인 case: maximizing the likelihood(log $$p(s' \bar a,s)$$).
       3. Gaussian distribution인 경우에는 동일함.
  3. Action을 선택하기 위해 _f(s, a)_ 를 통해 planning.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/2.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


> 1, 2 과정은 supervised learning과 유사하며, 3 과정은 지난번에 했던 내용(LQR, iLQR, DDP 사용).


* MBRL 0.5은 잘 동작? 
  * 그래도 많은 영역에서 동작!
  * 본질적으로 __system identification(입력과 출력을 모델링하는 것, physical parameters를 찾는것)이 classical robotics에서 동작하는 방식__ 이므로 동작.
  * 좋은 base policy를 설계하기 위해서 어느정도 주의를 기울여야한다.
      * 만약 물리학의 지식을 사용하여 dynamics representation을 수작업으로 처리하고 몇 가지 parameter를 fitting하면 특히 효과적.

* 그러나, 일반적으로는 동작하지 않음.
    * Imitation learning에 대해 언급했던 것과 동일한 이유(__Distribution mismatch 문제__)
        * 예제 목표: 산의 정상으로 가는것. 
            1. Base policy(π0) = random (빨간 선, 비탈길에서 걸음)
            2. Dynamics model 학습
                * 오른쪽으로 갈때 정상으로 올라감
            3. 항상 true는 아님.
                * 주어진 데이터를 기반으로 학습이 진행되기 때문.
        * Why this problem(왜 추락할까)?
            * Base policy는 some distribution $$P_{π_0}(s_t)$$로부터 data를 제공
                * 최종 $$p_{π_f}(s_t)$$와 base $$p_{π_0}(s_t)$$의 distribution이 동일하지 않음.
                * $$p_{π_0}(s_t)$$를 아무리 잘 generlization해도 보장할 수 없다.

* __새로운 training data를 활용__ 해 supervised learning을 통해 low generalization error을 가질 수 있다.
    * Arbitrary distribution으로 하게되면 보장할 수 없음.
    * 이러한 것이 DAGGER와 유사.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/3.png" class="lead"   style="width:640px; height:=360px"/>
</figure>


> Distribution mismatch 문제는 모델 class가 더욱 expressive할수록 더욱 악화(exacerbated).


---------

Can we do better?
- 성능 향상을 위해, 이전의 문제였던 distribution mismatch를 해결
  - $$p_{\pi_F}(s_t)$$에서 data를 수집하여 $$p_{\pi_o}(s_t) = p_{\pi_f}(s_t)$$ 조건 추가


* __Model-based reinforcement learning version 1.0__:* 
    1. Base policy $$π_0 (s_t, a_t)$$(e.g., random policy)을 구동하여 D = {$$(s, a, s’)_i$$} 수집.
    2. $$\sum_i \left \| f(s_i, a_i) - s' _i \right \| $$를 최소화하기 위해 dynamics model f(s, a) 학습.
    3. Action을 선택하기 위해 f(s, a)를 통해 planning.
    4. 위의 action을 실행하고 __결과 data {$$(s, a, s’)_j$$} 를 D에 추가__

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/4.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


* 문제점: 만약 실수를 하게 된다면(우리가 원한 state로 도달하지 않을 경우)?
    * Small error라도 축적되면 재앙이됨.
        * 전진으로 가는 것인데 조금씩 왼쪽으로 가게되면 많이 벗어나게됨
    * 해결책: __Model error를 replanning__.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/5.png" class="lead"   style="width:320px; height:=240px"/>
</figure>


-------

* __Model-based reinforcement learning version 1.5__:
        1. Base policy  $$π_0 (s_t, a_t)$$(e.g., random policy)을 구동하여 D = {$$(s, a, s’)_i$$} 수집.
        2. $$\sum_i \left \| f(s_i, a_i) - s' _i \right \| $$를 최소화하기 위해 dynamics model f(s, a) 학습.
        3. Action을 선택하기 위해 f(s, a)를 통해 planning
        4. __첫번째 계획된 action을 실행하고, 결과 state s’를 관측(MPC)__
            * Generate a new plan which will hopefully correct that mistake   
            * MPC가 유용한 이유는 모델이 완벽하지 않아도 replanning하여 policy의 실수에 대해 수정해준다.
        5. (s, a, s’)를 dataset D에 append


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/6.png" class="lead"   style="width:560px; height:=320px"/>
</figure>


* Replan하는 방법(How to replan?).
    * Replan을 할수록, 각 개별 완벽한 plan은 덜 필요하게 됨.
    * 짧은 horizon 사용 가능
    * 심지어 random sampling도 종종 잘동작.
        * huge long plan을 생성할수록 잘 동작


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/7.png" class="lead"   style="width:560px; height:=320px"/>
</figure>



* 단점: That seems like a lot of work…, 많은 연산 필요 - 3 step(replanning)
  * But, 다음주에 우측하단 방법을 사용하여 policy를 distill(증류)시킴(demonstration에서 supervising)
  * 이를 학습시키고 test할 때는 __cheaper policy 사용__

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/8.png" class="lead"   style="width:560px; height:=320px"/>
</figure>


---------

* 지금까지는 모델을 학습하고 __optimal control 기법을 이용하여 action 선택(computational power가 expensive)__
* __Model-based reinforcement learning version 2.0__:
    * __Policy를 backpropagate해서 직접 학습__ 할 수 있을까?
    * f(s,a)를 backpropagate해서 policy optimization (3번 과정, _second optimization_)
    * 아래의 그림에서는 deterministic이라고 가정,
        * Stochastic경우, _reparameterization trick_ 을 이용하여 generalization.
    * 그러나, NN은 종종 잘 동작하지 않음.
        * Vanishing gradient 문제.
        * recurrent NN을 사용하여 해결 가능.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/9.png" class="lead"   style="width:640px; height:=480px"/>
</figure>


--------

## Summary

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC11/10.png" class="lead"   style="width:480px; height:=360px"/>
</figure>

---------


# Reference
[CS294-112 Lecture11](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-11.pdf)