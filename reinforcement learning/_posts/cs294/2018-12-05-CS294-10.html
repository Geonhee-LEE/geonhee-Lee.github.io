<!DOCTYPE html>
    <html>
    <head>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
        <title>[CS294 - 112 정리] Lecture10 - Optimal control and planning</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
        <link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        
        <script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
    </head>
    <body>
        <hr>
<h2 id="layout-posttitle-%22rl%22categories-rltags-cs294comments-true">layout: post<br>
title:  &quot;RL&quot;<br>
categories: RL<br>
tags: CS294<br>
comments: true</h2>
<h1 id="cs294---112-%EC%A0%95%EB%A6%AC-lecture10---optimal-control-and-planning">[CS294 - 112 정리] Lecture10 - Optimal control and planning</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<p>{:.no_toc}</p>
<ol>
<li>this unordered seed list will be replaced by toc as unordered list<br>
{:toc}</li>
</ol>
<h2 id="todays-lecture">Today’s Lecture</h2>
<ol>
<li>Model-based reinforcement learning에 대한 소개</li>
<li>Dynamics를 안다면? 어떻게 decision을 생성할까?</li>
<li>Monte Carlo tree search(MCTS)</li>
<li>Trajectory optimization</li>
</ol>
<ul>
<li>Goals:
<ul>
<li>Discrete &amp; continous space에서 알고 있는 dynamics를 가지고 planning을 수행하는 방법</li>
</ul>
</li>
</ul>
<h2 id="recap">Recap</h2>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li>the reinforcement learning objective</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/2.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li>the model-free reinforcement learning</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/3.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<hr>
<h2 id="model-based-reinforcement-learning">Model-based reinforcement learning</h2>
<p>Transition dynamics를 알고 있다면?</p>
<ol>
<li>Dynamics를 알고 있는 경우:
<ul>
<li>Games(e.g., Go)</li>
<li>Easily modeled systems (e.g., navigating a car)</li>
<li>Simulated environments (e.g., simulated robots, video games)</li>
</ul>
</li>
<li>Dynamics를 학습할 수 있는 경우
<ul>
<li><strong>System identification</strong>:
<ul>
<li>알고있는 모델의 unknown parameter로 fitting(physical linkage) .</li>
</ul>
</li>
<li><strong>Learning</strong>:
<ul>
<li>Observed transition data를 관측하여 general-model(e.g., NN)을 fittng.</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>Dynamics를 안다면 더욱 쉽게 만들어 주는가?
<ul>
<li>Often, yes!</li>
<li>추가적인 learning을 할 필요없이 system을 <strong>제어 가능(optimization problem을 푸는 것)</strong>.</li>
</ul>
</li>
</ul>
<p><strong>Model-based reinforcement learning</strong></p>
<ol>
<li><strong>Mode-based RL</strong>: transition dynamics을 학습하고, 어떤 action을 할 것인지 찾기.</li>
<li><em>Today</em>: <strong>dynamics를 안다면 어떻게 decision</strong> 을 만들 수 있을까?
<ol>
<li>System dynamics의 perfect knowledge에서 action을 선택하는 방법.</li>
<li>Optimal control, trajectory optimization, planning</li>
</ol>
</li>
<li><em>Next topic: <strong>Unknown dynamics를 학습</strong> 하는 방법</em></li>
<li><em>Next topic: Dynamics 학습 후, <strong>policy 또한 학습</strong> 하는 방법(e.g. by imitating optimal control)</em></li>
</ol>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/4.png" class="lead"   style="width:480px; height:=240px"/>
</figure>
<p><strong>The objective</strong></p>
<p>policy 고려하지 않고 어떤 행동이 좋을 지 생각.</p>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/5.png" class="lead"   style="width:640px; height:=240px"/>
</figure>
<ul>
<li>Cost function:</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/6.png" class="lead"   style="width:360px; height:=180px"/>
</figure>
<ul>
<li>Constraint optimization problem(deterministic dynamics, 1:T):</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/7.png" class="lead"   style="width:360px; height:=180px"/>
</figure>
<hr>
<p><strong>The deterministic case vs stochastic case, open loop case vs closed loop case</strong></p>
<ul>
<li>The deterministic case:</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/8.png" class="lead"   style="width:480px; height:=240px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/9.png" class="lead"   style="width:480px; height:=120px"/>
</figure>
<ul>
<li>The stochastic open-loop case:
<ul>
<li>Probability distribution이 적용됨.</li>
<li>Conditional distribution, $$p(s \mid a)$$이 주어지고 expectation이 취해짐.</li>
<li>why is this <strong>suboptimal</strong>?
<ul>
<li>기대했던 state로 될 가능성이 적다(HW4).</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li></li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/10.png" class="lead"   style="width:480px; height:=240px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/11.png" class="lead"   style="width:480px; height:=240px"/>
</figure>
<ul>
<li>The closed loop case:</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/12.png" class="lead"   style="width:480px; height:=240px"/>
</figure>
<ul>
<li>The open-loop
<ul>
<li>문제점: 원하는 state로 가지못하면, 원하는 성능(목표)을 얻지 못한다.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/13.png" class="lead"   style="width:480px; height:=240px"/>
</figure>
<blockquote>
<p>따라서, deterministic이 아닌 경우(=stochastic)에는 closed loop를 사용해야만 한다.</p>
</blockquote>
<p><strong>The stochastic closed-loop case</strong></p>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/14.png" class="lead"   style="width:640px; height:=480px"/>
</figure>
<hr>
<h4 id="stochastic-optimization">Stochastic optimization</h4>
<ul>
<li><strong>Optimal control/ planning</strong>:</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/15.png" class="lead"   style="width:640px; height:=180px"/>
</figure>
<ul>
<li>
1.$$ Simplest method: __guess & check, “random shooting method”__
  1. Some distribution(e.g., uniform)에서 $$A_1,..., A_N$$ 선택
  2. Argmax$$_i J(A_i)$$ 기반하여 $$A_i$$ 선택

<ul>
<li>Some distribution이 아닌 더 좋은 방법은 무엇일까?</li>
<li>Continuous-valued input을 가진 cross-entropy method:
<ol>
<li>p(A)에서 $$A_1, ..., A_N$$ 샘플링</li>
<li>
J(A_1), ..., J(A_N)$$ 평가

</li>
<li>p(A)를 $$A_{i_1}, ..., A_{i_M}$$로 <strong>refit</strong>.</li>
</ol>
</li>
<li>일반적으로 Gaussian distribution 사용.
<ul>
<li>see also: CMA-ES(momentum을 가지고 CEM을 사용하는 방법 또한 존재)</li>
</ul>
</li>
<li>장점:
<ul>
<li>Parallelized하면 매우 빠름.</li>
<li>매우 단순.</li>
</ul>
</li>
<li>문제점:
<ul>
<li>매우 까다로운 dimensionality limit.</li>
<li>오직 <strong>open-loop planning</strong> 에서 사용가능.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/16.png" class="lead"   style="width:640px; height:=480px"/>
</figure>
<hr>
<h2 id="discrete-case-monte-carlo-tree-search-mcts-stochastic-method">Discrete case: Monte Carlo tree search (MCTS), Stochastic method</h2>
<ul>
<li>
<p>Search 문제와 같은 discrete planning</p>
<ul>
<li>모든 state, action에 대하여 값을 가지고 있는 것은 힘듬.</li>
</ul>
</li>
<li>
<p>Full tree없이 값을 근사하는 방법</p>
<ul>
<li>Policy를 이용하여 tree를 expansion.</li>
<li>모든 경로를 search할 수는 없다 - 어디를 첫 번째로 search?
<ul>
<li>다음 state에서의 score가 큰 것(방문 횟수와 받을 score의 합으로 계산됨)을 선택</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Intuition: <strong>best reward의 node 선택하지만, 방문했던 node는 덜 선호</strong>.</p>
</li>
<li>
<p>Generic MCTS</p>
<ol>
<li>Tree Policy($$s_{1l}$$) 사용하여 leaf $$s_l$$ 찾기</li>
<li>DefaultPolicy($$s_l$$) 사용하여 leaf 평가</li>
<li>
s_1$$ 및 $$s_l$$ 간의 tree의 모든 값들 update.

</li>
</ol>
</li>
<li>
<p>UCT Tree Policy($$s_t$$)</p>
<ul>
<li>
s_t$$ 가 not fully expanded, 새로운 $$a_t$$ 를 선택하고

</li>
<li>C는 hyper parameter.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/17.png" class="lead"   style="width:320px; height:=120px"/>
</figure>
<hr>
<h2 id="continous-action-space">Continous action space</h2>
<p>Can we use derivatives?</p>
<ul>
<li>Action sequence를 <strong>derivative하여 optimization</strong>.
<ul>
<li>Cost function을 사용한 constrained 문제, unconstrained 문제(elimination method 사용)가 있다</li>
</ul>
</li>
<li>일반적으로 back propagation을 통해 미분하고 최적화 진행.</li>
<li>실전에서는, 1차 미분방법(first order gradient descent)을 사용하면 매우 민감하기 때문에, chain rule을 적용하면 매우 작은 값이거나 매우 큰 값이 되기 때문에 <strong>2차 미분</strong> 을 사용</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC10/18.png" class="lead"   style="width:640px; height:=480px"/>
</figure>
<hr>
<h1 id="reference">Reference</h1>
<p><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-10.pdf">CS294-112 Lecture10</a></p>

    </body>
    </html>