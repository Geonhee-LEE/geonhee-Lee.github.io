<!DOCTYPE html>
    <html>
    <head>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
        <title>[CS294 - 112 정리] Lecture4 - Reinforcement Learning Introduction</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
        <link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        
        <script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
    </head>
    <body>
        <hr>
<h2 id="layout-posttitle-%22rl%22categories-rltags-mbrlcomments-true">layout: post<br>
title:  &quot;RL&quot;<br>
categories: RL<br>
tags: MBRL<br>
comments: true</h2>
<h1 id="cs294---112-%EC%A0%95%EB%A6%AC-lecture4---reinforcement-learning-introduction">[CS294 - 112 정리] Lecture4 - Reinforcement Learning Introduction</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<p>{:.no_toc}</p>
<ol>
<li>this unordered seed list will be replaced by toc as unordered list<br>
{:toc}</li>
</ol>
<h2 id="todays-lecture">Today’s Lecture</h2>
<ol>
<li>Definition of a Markov decision process</li>
<li>Definition of reinforcement learning</li>
<li>Anatomy of a RL algorithm</li>
<li>Brief overview of RL algorithm types</li>
</ol>
<ul>
<li>Goals:
<ul>
<li>definitions &amp; notation 이해.</li>
<li>중요한 RL 목적 이해.</li>
<li>이용 가능한 알고리즘의 요약.</li>
</ul>
</li>
</ul>
<h2 id="review">Review</h2>
<ul>
<li>Terminology &amp; notation
<ul>
<li>Sequential decision making 문제는 observation. action. policy를 포함.
<ul>
<li><strong>Policy</strong> 는 조건부 확률 분포이며, 일반적으로 observation이 주어지고 action이 도출된다. 파라미터화되어 있으며, 이 파라미터를 학습시키는 것이 RL의 목적이다. 바로 파라미터를 학습시키는 PG가 있다.</li>
<li>아래의 그래프에서 <strong>state</strong> 는 Markov property 의미를 만족, <strong>observation</strong> 은 Markov property를 만족하지 못함.</li>
<li>POMDPs과 Fully MDPs가 있는데, 일반적으로 fully MDPs로 가정한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li>
<p>Imitation Learning</p>
<ul>
<li>전문가의 데이터(observation, action)를 수집하고, supervised learning 알고리즘을 사용한다.
<ul>
<li>위와 같은 behavior cloning접근은 일반적으로 학습이 잘 되지않지만, Dagger를 사용하여 성능을 향상시켰다.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Reward functions</p>
<ul>
<li>전문가의 data가 없는 경우에, 어떻게 하면 효율적으로 policy를 학습할 수 있을까?</li>
<li>Reward를 이용하여 action이 좋은지 나쁜지를 찾을 수 있다.
<ul>
<li>Reward는 s, a로 구성, r(s, a)</li>
<li>Reward를 순간이 아닌 전체에 걸쳐 최대화 한다.</li>
<li>State, action, reward(r(s, a)), transition probability(p(s’$$\mid$$s,a))은 Markov decision process이라 한다.
<ul>
<li>MDP는 기본적으로 RL의 world의 정의.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="definitions">Definitions</h2>
<ul>
<li><strong>Markov chain</strong>
<ul>
<li>
T$$  - trainsition operator라고 한다. 현재 state에서 특정한 state로 가는 확률(조건부 확률분포).
  * 왜 operator라고 불리나?
      * Discrete space에서 $$T$$ 는 행렬이고, 확률의 vector($$\mu$$)에 대해 operator처럼 행동.
      * p는 state distribution이며, 0보다 큰 값을 가진다.
  * $$T _{i,j}$$ matrix는 $$p(s_{t+1} \mid s_t)$$, $$s_t$$에서 $$s_{t+1}$$로 전이하며, 간단한 linear equation으로 Markov chain forward $$(\overrightarrow{\mu_{t+1}} = T  \overrightarrow{\mu_t}$$ ) 표현.

</li>
<li>Markov property로 인해 $$s_3$$는 $$s_1$$와 독립적.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/2.png" class="lead"   style="width:560px; height:=480px"/>
</figure>
<ul>
<li><strong>Markov decision process(MDP)</strong>
<ul>
<li>Reward, action 개념 도입.</li>
<li>Action, state가 다음 state를 결정.</li>
<li>Transition Operator는 tensor(3D) 형태를 가짐.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/3.png" class="lead"   style="width:560px; height:=360px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/4.png" class="lead"   style="width:560px; height:=120px"/>
</figure>
<ul>
<li><strong>Partially observed Markov decision process(POMDP)</strong>
<ul>
<li>MDP의 generalization model.</li>
<li>Observation, emission probability 도입</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/5.png" class="lead"   style="width:560px; height:=560px"/>
</figure>
<hr>
<h2 id="the-goal-of-reinforcement-learning">The goal of reinforcement learning</h2>
<ul>
<li>목적:
<ul>
<li>보상을 최대로하는 policy(π)의 weight(θ)찾기.</li>
<li>파라미터 vector θ* 찾기, trajectory에 걸쳐 보상의 총합의 theta로 유도된 trajectory distribution하에서 expectation의 argmax.</li>
</ul>
</li>
<li>
p(s' \mid s, a)$$ 는 일반적으로 모르는 상태이며, 확률이고 다음 state을 지정.

<ul>
<li><strong>Expectation을 사용하는 이유</strong> : parameter vector인 state, action가 <strong>random variable이기 때문이고, 여기서의 분포를 사용</strong>.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/6.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li>아래의 항등식이 중요.
<ul>
<li>Markov chain를 분석하는 방법에 사용</li>
<li>Stationary distribution 찾는 방법</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/7.png" class="lead"   style="width:560px; height:=480px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/8.png" class="lead"   style="width:560px; height:=180px"/>
</figure>
<hr>
<p>두 MDP 경우를 살펴보자.</p>
<ul>
<li><strong>Finite horizon case: state-action marginal</strong>:
<ul>
<li>Expectation을 내부로 넣으면서, <strong>trajectory를 (s, a) pair</strong> 로 변경. 실제로 p(s,a)를 아는 것은 힘들지만(나중에 다룸), 매 time step에서 reward를 계산하기 위한 공식을 정의하기 위해 진행.</li>
<li>위에서 언급했듯, $$\tau$$ 는 state와 action의 trajectory $$s_1,a_1,…,s_T,a_T$$ 인데, Markov chain에서 $$\tau$$는 결국 transition probability 와 policy에 따라 결정.
<ul>
<li>즉, state-action간의 <strong>transition probability</strong> 는 $$p((s_{t+1}, a_{t+1}) \mid (s_t, a_t)) = p(s_{t+1} \mid s_t,a_t) \pi_\theta(a_{t+1} \mid s_{t+1})$$ 이고, $$p_\theta(s_t, a_t)$$ 로 표현 = <strong>state-action marginal</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/9.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li><strong>Infinite horizon case: stationary distribution</strong>:
<ul>
<li>
<p>T가 무한대로 간다면?</p>
<ul>
<li>Mild 가정 아래 Markov chain은 stationary distribution safety를 유도할 수 있다.
<ul>
<li>안정화시키는 것은 transition인 T가 k step이후에는 우측 하단처럼 될 수 있다.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>p(s,a)가 stationary distribution에 수렴하는가?</p>
<ul>
<li><strong>Stationary distribution</strong>: 시간이 지남에 따라 Markov chain에서 변하지 않은채 남아 있는 확률 분포.</li>
<li>Eigen value = 1, mild regularity condition(가벼운 규칙성 조건)에 있음.</li>
<li>결국에 <strong>특정값에 수렴</strong> 하게 될 것.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/10.png" class="lead"   style="width:560px; height:=480px"/>
</figure>
<ul>
<li>약간 수정됨(T가 무한대로 가기 때문에, T를 나누지만 stationary distribution으로 인해 소거됨)</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/11.png" class="lead"   style="width:360px; height:=180px"/>
</figure>
<hr>
<ul>
<li>Expectations and stochastic systems
<ul>
<li>RL에서는, <strong>expectation에 대해 항상 고려</strong>.</li>
<li>Reward function는 not smooth, not differentiable.
<ul>
<li>Expectation을 추가하여 smooth하게 만듬.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/12.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<hr>
<h2 id="algorithms">Algorithms</h2>
<ul>
<li>RL 알고리즘의 구조
<ul>
<li>Sample 생성(즉, policy를 토대로 agent 행동)</li>
<li>Fit a model/ estimate the return(sample들에 대한 평가, return 추정)</li>
<li>Policy 향상(policy를 업데이트하는 학습방식)</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/13.png" class="lead"   style="width:360px; height:=240px"/>
</figure>
<ul>
<li><strong>Policy gradient</strong>:
<ul>
<li>Policy를 direct update.</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/14.png" class="lead"   style="width:360px; height:=240px"/>
</figure>
<ul>
<li><strong>Model based learning:</strong>
<ul>
<li>State, action이 들어왔을 때, 다음 state 출력으로 가지는 예측 모델 $$f_\phi$$를 학습한다. $$f_\phi$$가 예측한 다음 state와 실제 다음 state간의 loss를 줄이도록 학습(Model-based RL).</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/15.png" class="lead"   style="width:360px; height:=240px"/>
</figure>
<ul>
<li>Which parts are expensive?
<ul>
<li>[주황색 박스] : MuJoCo등으로 시뮬레이션을 돌리는 경우는 현실의 시간보다 충분히 빠르게 학습을 돌릴 수 있으므로 큰 문제가 안되지만, 실제로 자율주행이나 로봇 등을 적용할 때 <strong>학습하는 경우는 시간이 많이 소요된다</strong>.</li>
<li>[초록색 박스] : reward를 계산하는 것은 쉽고 빠르게 구현가능하지만, 다음 state를 예측하는 <strong>모델 f를 학습하고 최적화하는 것은 오래걸린다</strong>. 물론 알고리즘에 따라 다르긴 하다.</li>
<li>[파란색 박스] :  model f나 reward을 통해 $$\pi_\theta$$를 학습, 알고리즘에 의존.</li>
</ul>
</li>
</ul>
<ul>
<li>Why is this not enough?
<ul>
<li>오직 deterministic dynamic, policy만 다룰 수 있다(?).</li>
<li>Gradient의 연산을 통해 학습하기 때문에 연속적인 state와 action만 다룰 수 있다.</li>
<li>Gradient vanishing 문제 등 최적화에 대한 문제들이 있다.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="stochastic-system">Stochastic system</h2>
<ul>
<li>Stochastic system으로 동작하게 하는 법
<ul>
<li>Conditional expectations</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/16.png" class="lead"   style="width:560px; height:=240px"/>
</figure>
<ul>
<li>Time step $$T$$까지 진행되고나면 <strong>trajectories</strong> 따른 reward $$r(s_1,a_1),r(s_2,a_2),r(s_3,a_3)...$$ 을 반환받고 그에 맞게 학습.</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/17.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li>앞으로 받을 보상을 예측할 수 있다면?
<ul>
<li>time step T까지(trajectory) 기다릴 필요없이 바로 policy를 <strong>앞으로의 보상에 맞게 학습</strong> 할 수 있을 것이다.</li>
<li>즉, 앞으로의 보상이 최대가 되는 action $$a_1$$이라 할때, policy가 $$a_1$$을 내뱉을 확률을 $$π(a_1 \mid s_1)=1$$이다.</li>
<li>이 때, 앞으로 받을 보상을 예측해주는 함수를 <strong>Q-function</strong>.</li>
</ul>
</li>
</ul>
<ul>
<li>Q-function
<ul>
<li>s에서 a를 취해 얻는 전체 보상의 합.</li>
</ul>
</li>
<li>Value function
<ul>
<li>s에서 얻는 전체 보상의 합.</li>
<li>Q-function으로 표현될 수 있음(V는 모든 action에 대한 Q의 합의 expectation(=average)).</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/18.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ul>
<li>만약 $$Q_pi(s_t, a_t) &gt; V _\pi (s_t)$$라면, action $$a_t$$는 다른 action들에 비해 <strong>평균 이상으로 좋은 action</strong> 이다. 이렇게 Q값에 기반해서 더 나은 행동을 하도록 policy를 학습할 수 있는 것이다
<ul>
<li>Idea1: Q를 안다면 policy 향상
<ul>
<li>argmax Q를 이용하여 정책갱신</li>
<li>갱신되는 정책은 무슨 정책이든 상관없이 더 좋아질 것.</li>
</ul>
</li>
<li>Idea2: 좋은 행동 a의 확률 증가하기 위한 gradient 계산.
<ul>
<li>Q &gt; V라면 평균보다 좋은 값이다.</li>
<li>따라서, 위의 조건의 action에 대해 probability를 높여주면 된다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/19.png" class="lead"   style="width:560px; height:=480px"/>
</figure>
<hr>
<h2 id="review-1">Review</h2>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/20.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<hr>
<h2 id="types-of-rl-algorithms">Types of RL algorithms</h2>
<ul>
<li><strong>Policy gradient</strong>: 직접적으로 아래의 목적함수를 미분</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/21.png" class="lead"   style="width:240px; height:=120px"/>
</figure>
<ul>
<li>
<p><strong>Value-based</strong>: (no explicit policy) optimal policy의 value function, Q function을 추정</p>
</li>
<li>
<p><strong>Actor-critic</strong>: 현재 policy의 value function, Q function 추정, policy 향상하기 위해 사용</p>
</li>
<li>
<p><strong>Model-based RL</strong>: transition model 추정, 그리고</p>
<ul>
<li>planning을 위해 모델사용(no explicit policy)</li>
<li>policy 향상을 위해 사용</li>
<li>Something else</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/22.png" class="lead"   style="width:360px; height:=360px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/23.png" class="lead"   style="width:480px; height:=360px"/>
</figure>
<ol>
<li>plan하기 위해 모델 사용(no policy)
<ol>
<li>trajectory 최적화/ 최적 제어(우선적으로 continuous space) - 필수적으로 action에 걸쳐 최적화하기 위해 backpropagation</li>
<li>discrete action space에서 discrete planning - e.g., Monte Carlo tree search</li>
</ol>
</li>
<li>Backpropagate gradient into the policy
<ol>
<li>동작하기 위해 몇가지 트릭 필요</li>
</ol>
</li>
<li>Value function을 배우기 위해 모델 사용
<ol>
<li>DP</li>
<li>Dyna</li>
</ol>
</li>
</ol>
<hr>
<ul>
<li>
<p>Trade-off</p>
<ul>
<li>여러가지 강화학습 알고리즘이 있지만, 풀고자 하는 문제의 성격이나 제약조건에 따라 적절한 알고리즘을 선택해야한다. 각각 알고리즘마다 장단점이 있다.</li>
</ul>
</li>
<li>
<p>Comparison: sample efficiency</p>
<ul>
<li><strong>샘플 효율성</strong> = 좋은 정책을 얻기 위해서 얼마나 많은 샘플이 필요한가?</li>
<li>가장 중요한 질문: <strong>Off policy 알고리즘</strong> ?
<ul>
<li><strong>Off policy</strong>: policy에서부터 새로운 샘플 생성없이 policy를 향상할 수 있다.</li>
<li>On policy: 각 time마다 약간이라도 policy가 변하면, 새로운 샘플을 생성해야 한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>More efficient 한 방법이 항상 더 좋은것은 아니다. simulation 비용이 많이 들지 않는다면, less efficient하더라도 더 나은 성능을 위해 많은 sample을 얻어서 학습하는 것이 효율적일 수 있다.</p>
</blockquote>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/26.png" class="lead" style="width:480px; height:=360px"/>
</figure>
<ul>
<li><strong>Stability &amp; ease of use</strong>
<ul>
<li>수렴하는지, 어디로 수렴하는지, 항상 수렴하는지에 대한 이슈다.</li>
<li>기존의 supervised learning에서는 gradient descent를 사용하기 때문에 대개 수렴하지만, RL에서는 대체로 gradient descent를 사용하지 않는다.
<ul>
<li><strong>Q-learning</strong>: fixed point iteration. 단순한 함수에는 수렴하지만, 복잡한 함수에 수렴하는지는 보장되지 않는다.</li>
<li><strong>Model-based RL</strong>: model이 reward에 대해 최적화되는 것이 아니다. 그저 다음 state를 예측하기 위해 학습한다. 더 나은 model이 더 나은 policy를 만들어주는지는 보장되어있지 않다.</li>
<li><strong>Policy gradient</strong>: objective에 대해 직접적으로 gradient descent를 사용하긴 하지만, local optima에 수렴할 가능성이 있다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/27.png" class="lead" style="width:480px; height:=360px"/>
</figure>
<ul>
<li>
<p>Comparison: assumptions</p>
<ul>
<li>Full observation or patially observation?
<ul>
<li>state를 모두 관측할 수 있는 경우(st=ot)가 있고, 부분만 관측 가능한 경우가 있다. 주로 value function fitting에서 full observability를 가정한다.</li>
</ul>
</li>
<li>Stochastic or deterministic?
<ul>
<li>Dynamic system 이나 policy에 대한 가정으로, stochastic은 분포를 따르긴 하지만 매번 할 때마다 결과가 달라질 수도 있고, Deterministic은 파라미터가 같다면 같은 입력일때 결과가 매번 같다.</li>
</ul>
</li>
<li>Continuous or discrete?
<ul>
<li>Action space나 state space가 연속적인지 불연속적인지에 따라서도 적용하는 알고리즘이 달라진다. 주로 continuous value function learning 이나 model-based RL 에서 continuity나 smoothness를 가정한다.</li>
</ul>
</li>
<li>Episodic or infinite horizon?
<ul>
<li>하나의(finite) episode로 구성되는 환경인지, 시간제약 없이 끝없이 이어지는(infinite) 환경인지에 따라서도 적용하는 알고리즘이 달라진다. 주로 policy gradient나 model-based RL에서 episodic learning을 가정한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/28.png" class="lead" style="width:480px; height:=360px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="file:////assets/img/CS294/LEC4/29.png" class="lead" style="width:480px; height:=360px"/>
</figure>
<hr>
<h1 id="reference">Reference</h1>
<p><a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf">CS294-112 Lecture4</a></p>

    </body>
    </html>