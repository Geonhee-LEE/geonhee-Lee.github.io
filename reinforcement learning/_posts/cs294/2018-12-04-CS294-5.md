---
layout: post
title:  "RL"
categories: RL
tags: CS294
comments: true
---

# [CS294 - 112 정리] Lecture5 -  Policy Gradients Introduction


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}



## Today’s Lecture


1. The policy gradient algorithm
2. What does the policy gradient do?
3. Basic variance reduction: causality
4. Basic variance reduction: baselines
5. Policy gradient examples

* Goals:
    * Understand policy gradient reinforcement learning
    * Understand practical considerations for policy gradients



## Review

* __The goal of reinforcement learning__
 

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>



* __$$\theta^*$$를 찾는 것은 두 가지 경우__.
    * Infinite horizon
    * Finite horizon


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/2.png" class="lead"   style="width:480px; height:=240px"/>
</figure>




* __Evaluating the objective__
  * 목적(objective function)을 최대화하는 것: trajectory의 총 보상의 합에 대한 기댓 값을 최대화.
    * 실제 값을 구하기 힘들기 때문에, 몬테카를로 기법(=무수히 많은 샘플을 통해 평균값을 구하는 방법)을 이용하여 근사.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/3.png" class="lead"   style="width:480px; height:=320px"/>
</figure>


## Policy gradient

* __Direct policy differentiation__
  * Policy gradient로 직접 policy의 weight를 갱신.
  * 따라서, objective function을 바로 미분하여 gradient descent 방법을 이용하여 최적점(최소지점)을 찾는다.
      * Log 특성을 사용하여, 미분 및 덧셈 원리 이용하여 단순화 및 tractable하게 변형.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/4.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


- p(s)는 $$\theta$$에 대한 값이 아니므로 미분하면 0.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/5.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


- __Evaluating the policy gradient__
  * __REINFORCE 알고리즘:__
      * $$1$$. policy에 따라 sample 저장 (generate samples)
      * $$2$$. objective function을 미분하여 위의 절차(direct policy differentiation)에 따라 policy gradient 계산(fit a model to estimate return)
      * $$3$$. gradient descent 방법을 통해 weight 갱신(improve the policy)

      * 그러나, REINFORCE 알고리즘은 잘 동작하지 않는다
          * Variance로 때문, variance reduction을 통해 향상 가능.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/6.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


* $$Log \pi_\theta$$ 는 무엇을 의미?
    * (Imitation learning에서 배웠던) _Maximum likelihood_ 와 상당히 유사
    * 좋은 샘플을 확률 증가, 좋지 않은 샘플은 확률 감소

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/7.png" class="lead"   style="width:560px; height:=360px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/8.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


--------


## What did we just do(지금까지 진행사항)?

* Gradient 계산
    * N개의 샘플링(몬테카를로 근사)을 통해 Trajectory 동안의 $$log \pi_\theta$$, reward들의 합의 곱으로 표현.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/9.png" class="lead"   style="width:560px; height:=360px"/>
</figure>

* 좋은 것을 더 자주하도록, 나쁜 것을 덜 하도록(maximum likelihood에 유사하기 때문에 $$log \pi_\theta$$가 의미하는 바를 이해할 수 있다) 유도.
* 간단하게 “trial and error”의 개념을 공식화 한다!


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/10.png" class="lead"   style="width:360px; height:=240px"/>
</figure>


* Partial observability
  * Markov property를 실제로는 사용되지 못한다!
  * 수정없이 POMDPs에서 policy gradient를 사용할 수 있다
      * state를 observation으로만 변경.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/11.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


--------

# Reference
[CS294-112 Lecture5](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)