---
layout: post
title:  "RL"
categories: RL
tags: MBRL
comments: true
---

# [CS294 - 112 정리] Lecture5 -  Policy Gradients Introduction


## Table of Contents
{:.no_toc}
1. this unordered seed list will be replaced by toc as unordered list
{:toc}



## Today’s Lecture


1. The policy gradient algorithm
2. What does the policy gradient do?
3. Basic variance reduction: causality
4. Basic variance reduction: baselines
5. Policy gradient examples

* Goals:
    * Understand policy gradient reinforcement learning
    * Understand practical considerations for policy gradients



## Review

* __The goal of reinforcement learning__
 

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/1.png" class="lead"   style="width:480px; height:=360px"/>
</figure>



* __$$\theta^*$$를 찾는 것은 두 가지 경우__.
    * Infinite horizon
    * Finite horizon


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/2.png" class="lead"   style="width:480px; height:=240px"/>
</figure>




* __Evaluating the objective__
  * 목적(objective function)을 최대화하는 것: trajectory의 총 보상의 합에 대한 기댓 값을 최대화.
    * 실제 값을 구하기 힘들기 때문에, 몬테카를로 기법(=무수히 많은 샘플을 통해 평균값을 구하는 방법)을 이용하여 근사.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/3.png" class="lead"   style="width:480px; height:=320px"/>
</figure>


## Policy gradient

* __Direct policy differentiation__
  * Policy gradient로 직접 policy의 weight를 갱신.
  * 따라서, objective function을 바로 미분하여 gradient descent 방법을 이용하여 최적점(최소지점)을 찾는다.
      * log 특성을 사용하여, 미분 및 덧셈 원리 이용하여 단순화 및 tractable하게 변형.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/4.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


- p(s)는 $$\theta$$에 대한 값이 아니므로 미분하면 0.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC5/5.png" class="lead"   style="width:560px; height:=480px"/>
</figure>



--------


## The goal of reinforcement learning

* 목적:
    * 보상을 최대로하는 policy(π)의 weight(θ)찾기.
    * 파라미터 vector θ* 찾기, trajectory에 걸쳐 보상의 총합의 theta로 유도된 trajectory distribution하에서 expectation의 argmax.
* $$p(s' \mid s, a)$$ 는 일반적으로 모르는 상태이며, 확률이고 다음 state을 지정.
* Trajectory(sequence, τ)는 state, action으로 구성, chain rule을 이용하여 policy와 transition operator를 product로 표현.
    * __Expectation을 사용하는 이유__ : parameter vector인 state, action가 __random variable이기 때문이고, 여기서의 분포를 사용__.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/6.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


* 아래의 항등식이 중요.
    * Markov chain를 분석하는 방법에 사용
    * Stationary distribution 찾는 방법


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/7.png" class="lead"   style="width:560px; height:=480px"/>
</figure>
<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/8.png" class="lead"   style="width:560px; height:=180px"/>
</figure>

---------

두 MDP 경우를 살펴보자.

- __Finite horizon case: state-action marginal__:
  - Expectation을 내부로 넣으면서, __trajectory를 (s, a) pair__ 로 변경. 실제로 p(s,a)를 아는 것은 힘들지만(나중에 다룸), 매 time step에서 reward를 계산하기 위한 공식을 정의하기 위해 진행.
  - 위에서 언급했듯, $$\tau$$ 는 state와 action의 trajectory $$s_1,a_1,…,s_T,a_T$$ 인데, Markov chain에서 $$\tau$$는 결국 transition probability 와 policy에 따라 결정. 
    - 즉, state-action간의 __transition probability__ 는 $$p((s_{t+1}, a_{t+1}) \mid (s_t, a_t)) = p(s_{t+1} \mid s_t,a_t) \pi_\theta(a_{t+1} \mid s_{t+1})$$ 이고, $$p_\theta(s_t, a_t)$$ 로 표현 = __state-action marginal__. 


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/9.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


- __Infinite horizon case: stationary distribution__:
  * T가 무한대로 간다면?
      * Mild 가정 아래 Markov chain은 stationary distribution safety를 유도할 수 있다.
          * 안정화시키는 것은 transition인 T가 k step이후에는 우측 하단처럼 될 수 있다.
        
  * p(s,a)가 stationary distribution에 수렴하는가?
      * __Stationary distribution__: 시간이 지남에 따라 Markov chain에서 변하지 않은채 남아 있는 확률 분포.
      * Eigen value = 1, mild regularity condition(가벼운 규칙성 조건)에 있음.
      * 결국에 __특정값에 수렴__ 하게 될 것.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/10.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


- 약간 수정됨(T가 무한대로 가기 때문에, T를 나누지만 stationary distribution으로 인해 소거됨)

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/11.png" class="lead"   style="width:360px; height:=180px"/>
</figure>


-------

- Expectations and stochastic systems
  * RL에서는, __expectation에 대해 항상 고려__.
  * Reward function는 not smooth, not differentiable.
    * Expectation을 추가하여 smooth하게 만듬.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/12.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


--------


## Algorithms

- RL 알고리즘의 구조
  * Sample 생성(즉, policy를 토대로 agent 행동)
  * Fit a model/ estimate the return(sample들에 대한 평가, return 추정)
  * Policy 향상(policy를 업데이트하는 학습방식)


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/13.png" class="lead"   style="width:360px; height:=240px"/>
</figure>


- __Policy gradient__:
  - Policy를 direct update.

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/14.png" class="lead"   style="width:360px; height:=240px"/>
</figure>


- __Model based learning:__
  - State, action이 들어왔을 때, 다음 state 출력으로 가지는 예측 모델 $$f_\phi$$를 학습한다. $$f_\phi$$가 예측한 다음 state와 실제 다음 state간의 loss를 줄이도록 학습(Model-based RL). 

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/15.png" class="lead"   style="width:360px; height:=240px"/>
</figure>


- Which parts are expensive?
  * [주황색 박스] : MuJoCo등으로 시뮬레이션을 돌리는 경우는 현실의 시간보다 충분히 빠르게 학습을 돌릴 수 있으므로 큰 문제가 안되지만, 실제로 자율주행이나 로봇 등을 적용할 때 __학습하는 경우는 시간이 많이 소요된다__.
  * [초록색 박스] : reward를 계산하는 것은 쉽고 빠르게 구현가능하지만, 다음 state를 예측하는 __모델 f를 학습하고 최적화하는 것은 오래걸린다__. 물론 알고리즘에 따라 다르긴 하다.
  * [파란색 박스] :  model f나 reward을 통해 $$\pi_\theta$$를 학습, 알고리즘에 의존.


* Why is this not enough?
  * 오직 deterministic dynamic, policy만 다룰 수 있다(?).
  * Gradient의 연산을 통해 학습하기 때문에 연속적인 state와 action만 다룰 수 있다.
  * Gradient vanishing 문제 등 최적화에 대한 문제들이 있다.


---------

## Stochastic system

- Stochastic system으로 동작하게 하는 법
  - Conditional expectations


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/16.png" class="lead"   style="width:560px; height:=240px"/>
</figure>


- Time step $$T$$까지 진행되고나면 __trajectories__ 따른 reward $$r(s_1,a_1),r(s_2,a_2),r(s_3,a_3)...$$ 을 반환받고 그에 맞게 학습.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/17.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


- 앞으로 받을 보상을 예측할 수 있다면? 
  - time step T까지(trajectory) 기다릴 필요없이 바로 policy를 __앞으로의 보상에 맞게 학습__ 할 수 있을 것이다. 
  - 즉, 앞으로의 보상이 최대가 되는 action $$a_1$$이라 할때, policy가 $$a_1$$을 내뱉을 확률을 $$π(a_1 \mid s_1)=1$$이다. 
  - 이 때, 앞으로 받을 보상을 예측해주는 함수를 __Q-function__.


* Q-function
    * s에서 a를 취해 얻는 전체 보상의 합.
* Value function
    * s에서 얻는 전체 보상의 합.
    * Q-function으로 표현될 수 있음(V는 모든 action에 대한 Q의 합의 expectation(=average)).


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/18.png" class="lead"   style="width:480px; height:=360px"/>
</figure>

* 만약 $$Q_pi(s_t, a_t) > V _\pi (s_t)$$라면, action $$a_t$$는 다른 action들에 비해 __평균 이상으로 좋은 action__ 이다. 이렇게 Q값에 기반해서 더 나은 행동을 하도록 policy를 학습할 수 있는 것이다
    * Idea1: Q를 안다면 policy 향상
        * argmax Q를 이용하여 정책갱신
        * 갱신되는 정책은 무슨 정책이든 상관없이 더 좋아질 것.
    * Idea2: 좋은 행동 a의 확률 증가하기 위한 gradient 계산.
        * Q > V라면 평균보다 좋은 값이다.
        * 따라서, 위의 조건의 action에 대해 probability를 높여주면 된다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/19.png" class="lead"   style="width:560px; height:=480px"/>
</figure>


-----

## Review


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/20.png" class="lead"   style="width:480px; height:=360px"/>
</figure>


-----

## Types of RL algorithms

* __Policy gradient__: 직접적으로 아래의 목적함수를 미분

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/21.png" class="lead"   style="width:240px; height:=120px"/>
</figure>


* __Value-based__: (no explicit policy) optimal policy의 value function, Q function을 추정


* __Actor-critic__: 현재 policy의 value function, Q function 추정, policy 향상하기 위해 사용


* __Model-based RL__: transition model 추정, 그리고
    * planning을 위해 모델사용(no explicit policy)
    * policy 향상을 위해 사용
    * Something else

  
<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/22.png" class="lead"   style="width:360px; height:=360px"/>
</figure>

<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/23.png" class="lead"   style="width:480px; height:=360px"/>
</figure>

1. plan하기 위해 모델 사용(no policy)
    1. trajectory 최적화/ 최적 제어(우선적으로 continuous space) - 필수적으로 action에 걸쳐 최적화하기 위해 backpropagation
    2. discrete action space에서 discrete planning - e.g., Monte Carlo tree search
2. Backpropagate gradient into the policy
    1. 동작하기 위해 몇가지 트릭 필요
3. Value function을 배우기 위해 모델 사용
    1. DP
    2. Dyna


----------

- Trade-off
  - 여러가지 강화학습 알고리즘이 있지만, 풀고자 하는 문제의 성격이나 제약조건에 따라 적절한 알고리즘을 선택해야한다. 각각 알고리즘마다 장단점이 있다.


- Comparison: sample efficiency
  * __샘플 효율성__ = 좋은 정책을 얻기 위해서 얼마나 많은 샘플이 필요한가?
  * 가장 중요한 질문: __Off policy 알고리즘__ ?
      * __Off policy__: policy에서부터 새로운 샘플 생성없이 policy를 향상할 수 있다.
      * On policy: 각 time마다 약간이라도 policy가 변하면, 새로운 샘플을 생성해야 한다.

> More efficient 한 방법이 항상 더 좋은것은 아니다. simulation 비용이 많이 들지 않는다면, less efficient하더라도 더 나은 성능을 위해 많은 sample을 얻어서 학습하는 것이 효율적일 수 있다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/26.png" class="lead" style="width:480px; height:=360px"/>
</figure>

- __Stability & ease of use__
  - 수렴하는지, 어디로 수렴하는지, 항상 수렴하는지에 대한 이슈다. 
  - 기존의 supervised learning에서는 gradient descent를 사용하기 때문에 대개 수렴하지만, RL에서는 대체로 gradient descent를 사용하지 않는다.
    * __Q-learning__: fixed point iteration. 단순한 함수에는 수렴하지만, 복잡한 함수에 수렴하는지는 보장되지 않는다.
    * __Model-based RL__: model이 reward에 대해 최적화되는 것이 아니다. 그저 다음 state를 예측하기 위해 학습한다. 더 나은 model이 더 나은 policy를 만들어주는지는 보장되어있지 않다.
    * __Policy gradient__: objective에 대해 직접적으로 gradient descent를 사용하긴 하지만, local optima에 수렴할 가능성이 있다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/27.png" class="lead" style="width:480px; height:=360px"/>
</figure>



- Comparison: assumptions

  * Full observation or patially observation?
    * state를 모두 관측할 수 있는 경우(st=ot)가 있고, 부분만 관측 가능한 경우가 있다. 주로 value function fitting에서 full observability를 가정한다.
  * Stochastic or deterministic?
    *   Dynamic system 이나 policy에 대한 가정으로, stochastic은 분포를 따르긴 하지만 매번 할 때마다 결과가 달라질 수도 있고, Deterministic은 파라미터가 같다면 같은 입력일때 결과가 매번 같다.
  * Continuous or discrete? 
    * Action space나 state space가 연속적인지 불연속적인지에 따라서도 적용하는 알고리즘이 달라진다. 주로 continuous value function learning 이나 model-based RL 에서 continuity나 smoothness를 가정한다.
  * Episodic or infinite horizon?
    *   하나의(finite) episode로 구성되는 환경인지, 시간제약 없이 끝없이 이어지는(infinite) 환경인지에 따라서도 적용하는 알고리즘이 달라진다. 주로 policy gradient나 model-based RL에서 episodic learning을 가정한다.


<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/28.png" class="lead" style="width:480px; height:=360px"/>
</figure>



<figure>
  <img alt="An image with a caption" src="/assets/img/CS294/LEC4/29.png" class="lead" style="width:480px; height:=360px"/>
</figure>



--------

# Reference
[CS294-112 Lecture4](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)