---
layout: post
title:  "Machine leaning"
categories: machine_learning
tags: 
comments: true
---

# Week1 - Motivation and Basics

### Table of Contents
{:.no_toc}
0. this unordered seed list will be replaced by toc as unordered list
{:toc}

-------------


- Weekly Objectives
  - Motivate the study on
    - Machinelearning,AI,Datamining....
    - Why? What?
    - Overview of the field
  - Short questions and answers on a story
    - What consists of machine learning?
    - MLE
    - MAP
  - Some basics
    - Probability
    - Distribution
    - And some rules...

-------------
### Maximum Likelihood Estimation

#### Binomial Distribution

- Binomialdistributionis
  - The __discrete probability distribution__
  - Of the number of successes in a sequence of __n independent yes/no experiments__, and each success has the probability of __$$\theta$$__
  - Also called a Bernoull iexperiment
- Flips are __i.i.d__
  - Independent events (처음 던질 때와 두 번째 던질 때 연관이 없는 상태)
  - Identically distributed according to binomial distribution (던질때마다 동일한 확률 분포를 가질 때)
- P(H) = $$\theta$$, P(T)=1 - $$\theta$$
- P(HHTHT)= $$\theta \cdot \theta  \cdot  (1- \theta)  \cdot  \theta  \cdot  (1- \theta) = \theta ^3(1 - \theta)^2$$
- Let's say
  - D as Data = H, H, T, H, T
    - n = 5
    - k = $$a_H$$ =3
    - p = $$\theta$$
  - p(D|$$\theta$$) = $$\theta ^{a_H} (1-\theta)^{a_T}$$

> How to get the probability?
  
#### Maximum Likelihood Estimation

-  p(D|$$\theta$$) = $$\theta ^{a_H} (1-\theta)^{a_T}$$ 
   -  Data: We have obseved the sequence data of D with $$a_H$$ and $$a_T$$
-  Our hypothesis:
   -  The gambiling result follows the binomial distribution of $$\theta$$
-  How to make our hypothesis strong(어떻게 '참'이다라고 말할 수 있을까)?
   -  Finding out a better distribution of the observation
      -  Can be done, but you need more rational.
      -  Binomial distribution보다 좋은 hypothesis 구조가 있다면, 이것도 가능하지만 더 많은 합리적 생각이 필요.
   -  Finding out the best candidate of $$\theta$$
      -  What's the condition to __make $$\theta$$ most plausible__ ?
      -  $$\theta$$를 최적화하여 hypothesis를 strong하게 만들 수 있음.

어떤 $$\theta$$를 선택했을 때 data를 가장 잘 설명할 수 있을까? 

- One candidate is the __Maximum Likelihood Estimation (MLE) of $$\theta$$
  - Choose $$\theta$$ that maximized the probability of obseverd data

$$
\begin{aligned}
\hat{\theta} = argmax_\theta P(D| \theta)
\end{aligned} 
$$


#### MLE calculation

- $$\hat{\theta} = argmax_\theta P(D| \theta) = argmax_\theta \theta ^{a_H} (1-\theta)^{a_T}$$
- This is going nowhere, so you use a trick (자승을 처리하기 까다로움)
  - Using the __log function__.
- $$\hat{\theta} = argmax_\theta ln P(D| \theta) = argmax_\theta ln(\theta ^{a_H} (1-\theta)^{a_T}) = argmax_\theta ( a_H ln\theta + a_T ln (1-\theta) )$$
- Then, thish is a __maximization problem__, so you use a __derivative__ that is set to zero
  - $$ \frac{d}{d\theta} ( a_H ln\theta + a_T ln (1-\theta) ) = 0$$
  - $$ \frac{a_H}{\theta} - \frac{a_T}{1-\theta} = 0$$
  - $$\theta = \frac{a_H}{a_T + a_H}$$
  - When $$\theta$$ is $$\frac{a_H}{a_T + a_H}$$, the $$\theta$$ becomes the __best candidate__ from the MLE perspective.
- $$\hat{\theta} = \frac{a_H}{a_H + a_T}$$

> Note
> > 압정을 던져 2/5, 3/5 확률이 나오는 것은 아래의 방법을 통해 계산.

> > binomial distribution, MLE, opmization(derivative).


#### Number of Trials & Simple Error Bound

- 시도 횟수가 많아 진다면 무엇이 달라질까?
  - 똑같은 $$\theta$$ 값이 나옴.  
  - Additional trials __reduce the error of our estimation__.
  - Right now, we have  $$\hat{\theta} = \frac{a_H}{a_H + a_T}, N = a_H + a_T$$
  - Let's say $$\theta^*$$ is the true parameter of the thumbrack flipping for any error, $$\epsilon > 0$$ 
  - We have a simple upper bound on the probability provided by Hoeffding's inequality

$$
\begin{aligned}
P(|\hat{\theta} - \theta ^* \geq \epsilon|) \leq 2 e ^{-2N\epsilon^2}
\end{aligned} 
$$

- Can you calculate the required number of trials, N?
  - To obtain $$\epsilon$$ = 0.1 with 0.01% case

> Note
> > This is  Probably Approximate Correct(PAC) learning



### Maximum a Posteriori Estimation

#### Incorporating Prior Knowledge
 
-  Merge the __previous knowledge__ to my trial
   -  $$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$$
      -  Posterior = (Likelihood x Prior Knowledge) / Normalizing Constant
   -  Already dealt with $$P(D|\theta) = \theta^{a_H} (1-\theta)^{a_T}$$
      -  $$\theta$$라고 가정하고 이러한 data가 관측될 확률. 
   -  $$P(\theta)$$ is part of __the pror knowledge__
      -  왜 압정의 앞/뒤가 이런 확률로 나올까?
      -  왜 사람이 이렇게 행동할까?
   -  P(D)는 많이 중요하지 않음
      -  사실 $$\theta$$에 대해 관심이 있음.
      -  여러번 시도를 해볼 수 있고, 요즘 data가 많음.
   - $$P(\theta|D)$$: Poseterior
     - Then, it is the conclusion influenced by the __data__ and the __prior knowledge__? Yes, it will be our future prior knowledge!
     - 즉, data가 주어졌을 때 $$\theta$$가 사실일 확률을 표현

#### More Formula from Bayes Viewpoint

- $$P(\theta|D) \propto P(D|\theta) P(\theta)$$
  - $$P(D|\theta) = \theta^{a_H} (1-\theta)^{a_T}$$
  - $$P(\theta)$$ = ????
    - 어떤 distribution(e.g. binomial distribution)에 의존하여 표현해야함.
    - 50%, 50% 사전 지식을 그대로 사용 불가.
  - 여러 가지 방법이 있으나, 하나의 방법:
    - __Beta distribution__, 특정범위내에서 [0,1]로 결합되어 있는 CDF로 표현하여 확률 성격을 잘 표현(아래그림참조)
    - $$P(\theta) = \frac{\theta ^{\alpha-1} (1-\theta)^{\beta -1 }}{B(\alpha, \beta)}, B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}, \Gamma (\alpha) = (\alpha -1 )!$$

<figure>
  <img alt="An image with a caption" src="/assets/img/ML/week1/1.png" class="lead" data-width="240" data-height="180" />
</figure>

- Response is
  - $$P(\theta) \propto P(D|\theta) P(\theta) \propto \theta ^{a_H} (1-\theta)^{a_T }\theta ^{\alpha-1} (1-\theta)^{\beta -1 } = \theta ^{a_H + \alpha-1} (1-\theta)^{a_T + \beta -1 }$$
  
#### Maximum a Posterior Estimation

- Goal: the most probable and more approximate $$\theta$$
- Previously in MLE, we found $$\theta$$ from $$\hat{\theta} = argmax_\theta P(D| \theta)$$ 
  - $$ P(D| \theta) = \theta ^{a_H} (1-\theta)^{a_T }$$ 
  - $$\hat{\theta} = argmax_\theta P(D| \theta)$$
- Now in MAP, we find $$\theta$$ from $$\hat{\theta} = argmax_\theta P(\theta|D)$$ 
  - $$ P( \theta|D) \propto \theta ^{a_H + \alpha-1} (1-\theta)^{a_T + \beta -1 }$$ 
  - $$\hat{\theta} = \frac{ \theta ^{a_H + \alpha-1} }{ \theta ^{a_H + \alpha + a_T + \beta -2} }$$
- The calculation is __same__ because anyhow it is the maximization.


> Note
> > Likelihood에 대해 maximization을 하는 것(MLE)이 아니고, posterior에 관하여 maximization(MAP).


------------

> Reference:
- [KAIST - Industrial & Systems Engineering Dept](https://aai.kaist.ac.kr/xe2/)
- [기계 학습, Machine Learning, AAILAB KAIST](https://www.youtube.com/playlist?list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz)